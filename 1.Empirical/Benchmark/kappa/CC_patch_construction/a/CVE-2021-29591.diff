diff --git a/tensorflow/lite/core/subgraph.cc b/tensorflow/lite/core/subgraph.cc
old mode 100644
new mode 100755
index 7f9dd2ce336..43496876a92
--- a/tensorflow/lite/core/subgraph.cc
+++ b/tensorflow/lite/core/subgraph.cc
@@ -156,6 +156,42 @@ const char* GetTFLiteOpName(const TfLiteRegistration& op_reg) {
   return tflite::EnumNamesBuiltinOperator()[op_reg.builtin_code];
 }
 
+// An utility test to detect if the subgraph is abused:
+// 1. Detects if recursion exists in the graph (recursion is not currently
+//    supported.
+// 2. Detects if the interpreter / subgraph is used in multiple subgraphs.
+//    Note: It's clearly documented that the interpreter / subgraph are not
+//    thread-safe. This serves as a check with possible false negatives
+//    unless we switch to atomic boolean flags.
+class SubgraphGuard {
+  public:
+   SubgraphGuard(TfLiteContext* context, bool* is_subgraph_in_use)
+       : is_subgraph_in_use_(is_subgraph_in_use) {
+     if (*is_subgraph_in_use_) {
+       TF_LITE_KERNEL_LOG(
+           context,
+           "Subgraph is already in use. Using an interpreter or a subgraph in "
+           "multiple threads is not supported. Recursion in the graph is not "
+           "supported.");
+       status_ = kTfLiteError;
+     } else {
+       *is_subgraph_in_use_ = true;
+     }
+   }
+   ~SubgraphGuard() {
+     // If tht original status was OK, recover the boolean flag.
+     if (status_ == kTfLiteOk) {
+       *is_subgraph_in_use_ = false;
+     }
+   }
+ 
+   TfLiteStatus status() const { return status_; }
+ 
+  private:
+   TfLiteStatus status_ = kTfLiteOk;
+   bool* is_subgraph_in_use_;
+ };
+ 
 }  // namespace
 
 // A trivial implementation of GraphInfo around the Interpreter.
@@ -677,6 +713,11 @@ TfLiteStatus Subgraph::AllocateTensors() {
     }
     return kTfLiteOk;
   }
+  // Note `AllocateTensors` sometimes calls itself recursively above
+  // for delegates. Therefore only the logic below need to be guarded
+  // by `SubgraphGuard`.
+  SubgraphGuard guard(&context_, &is_subgraph_in_use_);
+  TF_LITE_ENSURE_OK(&context_, guard.status());
 
   next_execution_plan_index_to_prepare_ = 0;
   next_execution_plan_index_to_plan_allocation_ = 0;
@@ -1014,6 +1055,8 @@ TfLiteStatus Subgraph::PrepareOpsAndTensors() {
 }
 
 TfLiteStatus Subgraph::Invoke() {
+  SubgraphGuard guard(&context_, &is_subgraph_in_use_);
+  TF_LITE_ENSURE_OK(&context_, guard.status());
   if (!consistent_) {
     ReportError("Invoke called on model that is not consistent.");
     return kTfLiteError;
diff --git a/tensorflow/lite/core/subgraph.h b/tensorflow/lite/core/subgraph.h
old mode 100644
new mode 100755
index 793d1de792a..d6912d7bed0
--- a/tensorflow/lite/core/subgraph.h
+++ b/tensorflow/lite/core/subgraph.h
@@ -759,6 +759,10 @@ class Subgraph {
   // Whether memory planner should be instantiated to retain intermediates for
   // debugging.
   bool preserve_all_tensors_ = false;
+
+  // Whether the subgraph is currently in use (e.g. running the `Invoke`
+  // or `AllocateTensors` functions).
+  bool is_subgraph_in_use_ = false;
 };
 
 }  // namespace tflite
diff --git a/tensorflow/lite/kernels/while.cc b/tensorflow/lite/kernels/while.cc
old mode 100644
new mode 100755
index e05959fe2a6..c69466cb340
--- a/tensorflow/lite/kernels/while.cc
+++ b/tensorflow/lite/kernels/while.cc
@@ -139,6 +139,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
   TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());
   TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());
 
+
   Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();
   Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();
 
